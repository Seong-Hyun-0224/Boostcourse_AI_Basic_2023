{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f15661",
   "metadata": {},
   "source": [
    "# Lab 04-1. Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ea3d0",
   "metadata": {},
   "source": [
    "- 지금까지 다룬 선형회귀가 1개의 정보를 통해 1개의 추측값을 구하는 Simple Linear Regression이라면,\n",
    "- 오늘은 여러개, 복수의 정보를 통해 1개의 추측값을 구하는 Mulitvariate Linear Rregression을 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d470184",
   "metadata": {},
   "source": [
    "- 입력변수가 3개, 4개까지야 뭐 y = W1*x1 + W2*x2 + W3*x3 + b 같은 걸로 정의할 수 있지만,\n",
    "- 입력변수가 100개, 200개, 300개까지 다양한 dataset들로 늘어난다면...?\n",
    "- 그걸 해결하는게 Matrix, matmul함수를 통해 할 수 있다.\n",
    "- 행렬곱을 이용해 x를 하나만 쓰고도 matmul행렬곱 함수를 이용해 각 vector의 element마다 W를 곱해주는 것으로 식도 간단하게 만들고, 연산도 간단하게 할 수 있게 해결!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b378562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 85],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83679094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([70.7484, 81.4051, 80.1995, 87.3474, 62.0861]) Cost: 9097.098633\n",
      "Epoch    2/20 hypothesis: tensor([109.9108, 126.4677, 124.5944, 135.6989,  96.4548]) Cost: 2795.581299\n",
      "Epoch    3/20 hypothesis: tensor([131.5885, 151.4128, 149.1694, 162.4640, 115.4803]) Cost: 864.642395\n",
      "Epoch    4/20 hypothesis: tensor([143.5875, 165.2216, 162.7731, 177.2798, 126.0126]) Cost: 272.952332\n",
      "Epoch    5/20 hypothesis: tensor([150.2288, 172.8659, 170.3035, 185.4811, 131.8434]) Cost: 91.639618\n",
      "Epoch    6/20 hypothesis: tensor([153.9043, 177.0979, 174.4720, 190.0208, 135.0716]) Cost: 36.076332\n",
      "Epoch    7/20 hypothesis: tensor([155.9382, 179.4409, 176.7796, 192.5337, 136.8593]) Cost: 19.045399\n",
      "Epoch    8/20 hypothesis: tensor([157.0632, 180.7382, 178.0569, 193.9245, 137.8493]) Cost: 13.821927\n",
      "Epoch    9/20 hypothesis: tensor([157.6851, 181.4567, 178.7641, 194.6944, 138.3980]) Cost: 12.216333\n",
      "Epoch   10/20 hypothesis: tensor([158.0286, 181.8548, 179.1555, 195.1203, 138.7023]) Cost: 11.719545\n",
      "Epoch   11/20 hypothesis: tensor([158.2179, 182.0756, 179.3722, 195.3561, 138.8714]) Cost: 11.562407\n",
      "Epoch   12/20 hypothesis: tensor([158.3219, 182.1982, 179.4922, 195.4864, 138.9655]) Cost: 11.509367\n",
      "Epoch   13/20 hypothesis: tensor([158.3787, 182.2664, 179.5586, 195.5584, 139.0182]) Cost: 11.488308\n",
      "Epoch   14/20 hypothesis: tensor([158.4093, 182.3045, 179.5954, 195.5981, 139.0479]) Cost: 11.476929\n",
      "Epoch   15/20 hypothesis: tensor([158.4254, 182.3260, 179.6158, 195.6200, 139.0650]) Cost: 11.468575\n",
      "Epoch   16/20 hypothesis: tensor([158.4336, 182.3383, 179.6272, 195.6320, 139.0750]) Cost: 11.461179\n",
      "Epoch   17/20 hypothesis: tensor([158.4373, 182.3454, 179.6334, 195.6385, 139.0811]) Cost: 11.454027\n",
      "Epoch   18/20 hypothesis: tensor([158.4385, 182.3497, 179.6369, 195.6420, 139.0851]) Cost: 11.447012\n",
      "Epoch   19/20 hypothesis: tensor([158.4384, 182.3525, 179.6389, 195.6438, 139.0879]) Cost: 11.439976\n",
      "Epoch   20/20 hypothesis: tensor([158.4375, 182.3544, 179.6400, 195.6446, 139.0900]) Cost: 11.433025\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fd483",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "- 인공신경망, 즉 hypothesis를 간편하게 쓰기 위한 pytorch 모듈\n",
    "- nn.Module을 상속해서 hyopthesis로 쓸 model 클래스 선언할 수 있음(아래 예시 참고)\n",
    "- nn.Linear(3, 1)\n",
    "-  입력 차원 : 3\n",
    "-  출력 차원 : 1\n",
    "- hypothesis계산은 forward()에서!\n",
    "- Gradient 계산은 PyTorch가 알아서 해준다 backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25666c",
   "metadata": {},
   "source": [
    "## F.mse_loss\n",
    "- torch.nn.functional에서 제공하는 loss function 사용\n",
    "- 쉽게 다른 loss와 교체 가능!(l1.loss, smooth_l1_loss 등...)\n",
    "- cost 계산이 간편해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f826b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42673d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch    1/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch    2/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch    3/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch    4/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch    5/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch    6/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch    7/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch    8/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch    9/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   10/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   11/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   12/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   13/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   14/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   15/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   16/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   17/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   18/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   19/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n",
      "Epoch   20/20 hypothesis: tensor([48.1312, 59.2521, 57.7312, 62.1733, 46.2006]) Cost: 13727.607422\n"
     ]
    }
   ],
   "source": [
    "# nn.Module을 사용한 MultivariateLinearRegressionModel 클래스 선언\n",
    "# F.mse_loss\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 85],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "# W = torch.zeros((3, 1), requires_grad=True)\n",
    "# b = torch.zeros(1, requires_grad=True)\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    # hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    # cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
